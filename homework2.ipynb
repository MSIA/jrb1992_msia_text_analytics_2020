{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # The preprocessing includes tokenization and normalization.\n",
    "    # Output the preprocessing results as a text file, each line containing a single document, with normalized tokens separated by a single white space.    \n",
    "    \n",
    "    # 1. Tokenization\n",
    "    tokenized = []\n",
    "    for t in text:\n",
    "        tokenized.append(nltk.word_tokenize(t))   \n",
    "    \n",
    "    # 2. Normalization\n",
    "    normalized = []\n",
    "    separated = []\n",
    "    for word in tokenized:\n",
    "        # Convert to lower cases and remove non-alphabetic chars\n",
    "        alphabetic_lower = [i.lower() for i in word if i.isalpha()]\n",
    "        normalized.append(alphabetic_lower)\n",
    "        # Normalizaed tokens separated by a single white space\n",
    "        separated.append(' '.join(alphabetic_lower))\n",
    "    \n",
    "    # 3. Output the preprocessing results as a text file\n",
    "    with open('Processed_Output.txt', 'w') as f:\n",
    "        for line in separated:\n",
    "            # Each line containing a single document\n",
    "            f.write('%s\\n' % line)\n",
    "    \n",
    "    return normalized        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_neighbors(model, word_vec):\n",
    "    # Find 5 closest neighbors for list of words under given models\n",
    "    ls = []\n",
    "    for word in word_vec:\n",
    "        neighbors5 = model.wv.most_similar(word)[:5]\n",
    "        ls.append(neighbors5)\n",
    "    return ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import text corpus\n",
    "filepath = os.path.abspath(os.getcwd()) + '/20_newsgroups/alt.atheism/*'\n",
    "files = []\n",
    "# .glob() retrieves the list of files matching the specified pattern in the file_pattern parameter\n",
    "for i in glob.glob(filepath):\n",
    "    with open(i, 'r', encoding = \"utf8\", errors = \"ignore\") as f:\n",
    "        data = f.read()\n",
    "        files.append(data)\n",
    "        f.close()\n",
    "# Preprocess the text\n",
    "normalized = preprocess(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare word2vec models with different parameters\n",
    "# cbow\n",
    "model_cbow1 = Word2Vec(normalized, size = 50, window=5,  workers=4, sg=0)\n",
    "model_cbow2 = Word2Vec(normalized, size = 50, window=50, workers=4, sg=0)\n",
    "model_cbow3 = Word2Vec(normalized, size = 200, window=50, workers=4, sg=0)\n",
    "# skip gram\n",
    "model_skipgram = Word2Vec(normalized, size = 50, window=50, workers=4, sg=1)\n",
    "# list of models\n",
    "models = [model_cbow1, model_cbow2, model_cbow3, model_skipgram]\n",
    "models_name = ['model_cbow1', 'model_cbow2', 'model_cbow3', 'model_skipgram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand-pick 10 words and find 5 closest neighbors for these words under given models\n",
    "word_vector = ['central', 'good', 'bad', 'fact', 'evolve', 'large', 'political', 'god', 'science', 'assumptions']\n",
    "\n",
    "# Find 5 closest neighbors for list of words under given models and save results to csv\n",
    "for i in range(len(models)):\n",
    "    neighbors = find_closest_neighbors(models[i], word_vector)\n",
    "    res_df = pd.DataFrame(neighbors, index = word_vector, columns = ['1st', '2nd', '3rd', '4th', '5th'])\n",
    "    res_df.to_csv(models_name[i] + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
