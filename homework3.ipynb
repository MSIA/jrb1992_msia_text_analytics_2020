{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# # from sklearn.feature_extraction.text import TfidTransformer\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summary statistics should include\n",
    "# the number of documents, number of labels, label distribution, average / mean word length of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(reviews):\n",
    "    star_ls = []\n",
    "    text_ls = []\n",
    "    doc_len_ls = []\n",
    "    for review in reviews:\n",
    "        dat = json.loads(review)\n",
    "        star = dat['stars']\n",
    "        text = dat['text']\n",
    "        star_ls.append(star)\n",
    "        text_ls.append(text)\n",
    "        doc_len_ls.append(len(word_tokenize(text)))\n",
    "    return star_ls, text_ls, doc_len_ls  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "lines = open('yelp_academic_dataset_review.json', encoding=\"utf8\").readlines()[:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_ls, text_ls, doc_len_ls = pre_process(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents\n",
    "num_doc = len(lines)\n",
    "# Number of labels\n",
    "num_labels = len(set(star_ls))\n",
    "# Label distribution\n",
    "distribution = Counter(star_ls)\n",
    "# average / mean word length of documents\n",
    "avg_len = mean(doc_len_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  500000\n",
      "Number of labels:  5\n",
      "Label distribution:  Counter({5.0: 220375, 4.0: 112802, 1.0: 70468, 3.0: 55778, 2.0: 40577})\n",
      "Average word length of documents:  123.175352\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents: \", num_doc)\n",
    "print(\"Number of labels: \", num_labels)\n",
    "print(\"Label distribution: \", distribution)\n",
    "print(\"Average word length of documents: \", avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenization(text, stopwords):\n",
    "    # Apply nltk word tokenization, convert to lower cases, remove non-alphabetic chars and stopwords\n",
    "    review_tokenized = [i.lower() for i in word_tokenize(text) if i.isalpha() and i not in stopwords]\n",
    "    return review_tokenized\n",
    "\n",
    "def cleaning(stars, texts):\n",
    "    # List of stopwords that do not add much meaning to a sentence\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    texts_tokenized = [tokenization(i, stop_words) for i in texts]\n",
    "    cleaned_df = pd.DataFrame({'star': stars, 'text': texts_tokenized})\n",
    "    # Create binary lables: 1 for ratings > 3, 0 for ratings <= 3. This variable is created to apply logistic regression on multi-label class.\n",
    "    cleaned_df['binary_label'] = cleaned_df.apply(lambda x: 1 if x['star'] > 3 else 0, axis=1)\n",
    "    # Remove NAs\n",
    "    cleaned_df = cleaned_df.dropna()\n",
    "    return cleaned_df\n",
    "\n",
    "cleaned_df = cleaning(star_ls, text_ls)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 rows of cleaned dataset: \n",
      "    star                                               text  binary_label\n",
      "0   2.0  [as, someone, worked, many, museums, i, eager,...             0\n",
      "1   1.0  [i, actually, horrified, place, still, busines...             0\n",
      "2   5.0  [i, love, deagan, i, i, really, the, atmospher...             1\n",
      "3   1.0  [dismal, lukewarm, texmex, glop, mumbly, uneng...             0\n",
      "4   4.0  [oh, happy, day, finally, canes, near, casa, y...             1\n",
      "5   5.0  [this, definitely, favorite, fast, food, sub, ...             1\n",
      "6   5.0  [really, good, place, simple, decor, amazing, ...             1\n",
      "7   5.0  [awesome, office, staff, professional, friendl...             1\n",
      "8   5.0  [most, delicious, authentic, italian, i, us, y...             1\n",
      "9   4.0  [i, twice, very, nice, laid, back, i, tried, w...             1\n"
     ]
    }
   ],
   "source": [
    "print(\"The first 10 rows of cleaned dataset: \\n\", cleaned_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataframe successfully saved into CSV as 'cleaned_reviews.csv'\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.to_csv('cleaned_reviews.csv')\n",
    "print(\"Cleaned dataframe successfully saved into CSV as 'cleaned_reviews.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3 or more sets of experiments,\n",
    "# varying the bag-of-word representation (1gram vs 1gram+2gram),\n",
    "# and 2 or more hyperparameter settings.\n",
    "\n",
    "# Write a several-sentence rationale for your choice of parameters. \n",
    "# Produce a table summarizing your experiments: parameter values, BOW representation, results \n",
    "# in terms of Precision, Recall, F1-score, micro-averaged F1-score, or Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['text'], data['binary_label'], test_size=0.33, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_transform(vectorizer, train, test):\n",
    "    features_train = vectorizer.fit_transform(train)\n",
    "    features_test = vectorizer.fit_transform(test)\n",
    "    return features_train, features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_metrics(test, pred):\n",
    "    print(\"Precision:\", precision_score(test, pred))\n",
    "    print(\"Recall: \", recall_score(test, pred))\n",
    "    print(\"F1 score: \", f1_score(test, pred))\n",
    "    print(\"Accuracy: \", accuracy_score(test, pred))\n",
    "    print(\"Micro-averaged F1-score: \", f1_score(test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "# Read data in from CSV\n",
    "data = pd.read_csv(\"cleaned_reviews.csv\")\n",
    "data = data.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "# Split data into test and train sets\n",
    "X_train, X_test, y_train, y_test = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1gram with C=0.5 and max_iter = 400\n",
      "Precision: 0.7611896806278654\n",
      "Recall:  0.7122045204620432\n",
      "F1 score:  0.7358828086484963\n",
      "Accuracy:  0.6590727272727273\n",
      "Micro-averaged F1-score:  0.6590727272727273\n"
     ]
    }
   ],
   "source": [
    "# 1gram, C=0.5, max_iter = 400\n",
    "tfidf_1gram = TfidfVectorizer(min_df = 100, max_features = 300, ngram_range=(1,1), sublinear_tf = True, stop_words = 'english')\n",
    "train_x_1gram, test_x_1gram = tfidf_transform(tfidf_1gram, X_train, X_test)\n",
    "\n",
    "print(\"Results for 1gram with C=0.5 and max_iter = 400\")\n",
    "lr1 = LogisticRegression(C=0.5, random_state=42, max_iter=400)\n",
    "model1 = lr1.fit(train_x_1gram, y_train)\n",
    "pred1 = model1.predict(test_x_1gram)\n",
    "score_metrics(y_test, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1gram with C=5 and max_iter = 400\n",
      "Precision: 0.7616544879059044\n",
      "Recall:  0.7085692474075959\n",
      "F1 score:  0.7341534955766795\n",
      "Accuracy:  0.6577878787878788\n",
      "Micro-averaged F1-score:  0.6577878787878788\n"
     ]
    }
   ],
   "source": [
    "# 1gram, C=5, max_iter = 400\n",
    "print(\"Results for 1gram with C=5 and max_iter = 400\")\n",
    "lr2 = LogisticRegression(C=5, random_state=42, max_iter=400)\n",
    "model2 = lr2.fit(train_x_1gram, y_train)\n",
    "pred2 = model2.predict(test_x_1gram)\n",
    "score_metrics(y_test, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1gram2gram with C=0.5 and max_iter = 400\n",
      "Precision: 0.8275998520391924\n",
      "Recall:  0.8743376986903929\n",
      "F1 score:  0.8503270284603147\n",
      "Accuracy:  0.794739393939394\n",
      "Micro-averaged F1-score:  0.794739393939394\n"
     ]
    }
   ],
   "source": [
    "# 1gram+2gram, C=0.5, max_iter = 400\n",
    "tfidf_1gram2gram = TfidfVectorizer(min_df = 100, max_features = 100, ngram_range=(1,2), sublinear_tf = True, stop_words = 'english')\n",
    "train_x_1gram2gram, test_x_1gram2gram = tfidf_transform(tfidf_1gram2gram, X_train, X_test)\n",
    "\n",
    "print(\"Results for 1gram2gram with C=0.5 and max_iter = 400\")\n",
    "lr3 = LogisticRegression(C=0.5, random_state=42, max_iter=400)\n",
    "model3 = lr3.fit(train_x_1gram2gram, y_train)\n",
    "pred3 = model3.predict(test_x_1gram2gram)\n",
    "score_metrics(y_test, pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1gram2gram with C=5 and max_iter = 100\n",
      "Precision: 0.8278435896552911\n",
      "Recall:  0.8739105541064953\n",
      "F1 score:  0.8502535490231621\n",
      "Accuracy:  0.7947212121212122\n",
      "Micro-averaged F1-score:  0.7947212121212122\n"
     ]
    }
   ],
   "source": [
    "# 1gram+2gram, C=5, max_iter = 100\n",
    "print(\"Results for 1gram2gram with C=5 and max_iter = 100\")\n",
    "lr4 = LogisticRegression(C=5, random_state=42, max_iter=100)\n",
    "model4 = lr4.fit(train_x_1gram2gram, y_train)\n",
    "pred4 = model4.predict(test_x_1gram2gram)\n",
    "score_metrics(y_test, pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['text'], data['binary_label'], test_size=0.33, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_transform(vectorizer, train, test):\n",
    "    features_train = vectorizer.fit_transform(train)\n",
    "    features_test = vectorizer.fit_transform(test)\n",
    "    return features_train, features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_metrics(test, pred):\n",
    "    print(\"Precision:\", precision_score(test, pred))\n",
    "    print(\"Recall: \", recall_score(test, pred))\n",
    "    print(\"F1 score: \", f1_score(test, pred))\n",
    "    print(\"Accuracy: \", accuracy_score(test, pred))\n",
    "    print(\"Micro-averaged F1-score: \", f1_score(test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "# Read data in from CSV\n",
    "data = pd.read_csv(\"cleaned_reviews.csv\")\n",
    "data = data.drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "# Split data into test and train sets\n",
    "X_train, X_test, y_train, y_test = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1gram with C=0.5 and loss=hinge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vesper7367/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7600279552322549\n",
      "Recall:  0.7017167576999628\n",
      "F1 score:  0.7297092957320531\n",
      "Accuracy:  0.6533333333333333\n",
      "Micro-averaged F1-score:  0.6533333333333333\n"
     ]
    }
   ],
   "source": [
    "# 1gram, C=0.5, loss=hinge\n",
    "tfidf_1gram = TfidfVectorizer(min_df = 100, max_features = 300, ngram_range=(1,1), sublinear_tf = True, stop_words = 'english')\n",
    "train_x_1gram, test_x_1gram = tfidf_transform(tfidf_1gram, X_train, X_test)\n",
    "\n",
    "print(\"Results for 1gram with C=0.5 and loss=hinge\")\n",
    "svm1 = LinearSVC(C=0.5, loss = 'hinge', random_state=42)\n",
    "model1 = svm1.fit(train_x_1gram, y_train)\n",
    "pred1 = model1.predict(test_x_1gram)\n",
    "score_metrics(y_test, pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1gram with C=0.5 and loss=squared_hinge\n",
      "Precision: 0.7617140959784371\n",
      "Recall:  0.7088600692519517\n",
      "F1 score:  0.7343372671596895\n",
      "Accuracy:  0.657969696969697\n",
      "Micro-averaged F1-score:  0.657969696969697\n"
     ]
    }
   ],
   "source": [
    "# 1gram, C=0.5, loss=squared_hinge\n",
    "\n",
    "print(\"Results for 1gram with C=0.5 and loss=squared_hinge\")\n",
    "svm2 = LinearSVC(C=0.5, loss = 'squared_hinge', random_state=42)\n",
    "model2 = svm2.fit(train_x_1gram, y_train)\n",
    "pred2 = model2.predict(test_x_1gram)\n",
    "score_metrics(y_test, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1gram2gram with C=0.5 and loss=hinge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vesper7367/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8282997088365522\n",
      "Recall:  0.8738651131933147\n",
      "F1 score:  0.8504725387959438\n",
      "Accuracy:  0.7950848484848485\n",
      "Micro-averaged F1-score:  0.7950848484848485\n"
     ]
    }
   ],
   "source": [
    "# 1gram+2gram, C=0.5, loss=hinge\n",
    "tfidf_1gram2gram = TfidfVectorizer(min_df = 100, max_features = 100, ngram_range=(1,2), sublinear_tf = True, stop_words = 'english')\n",
    "train_x_1gram2gram, test_x_1gram2gram = tfidf_transform(tfidf_1gram2gram, X_train, X_test)\n",
    "\n",
    "print(\"Results for 1gram2gram with C=0.5 and loss=hinge\")\n",
    "svm3 = LinearSVC(C=0.5, loss = 'hinge', random_state=42)\n",
    "model3 = svm3.fit(train_x_1gram2gram, y_train)\n",
    "pred3 = model3.predict(test_x_1gram2gram)\n",
    "score_metrics(y_test, pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 1gram2gram with C=5 and loss=squared_hinge\n",
      "Precision: 0.826364657367478\n",
      "Recall:  0.8759917479301664\n",
      "F1 score:  0.8504548382258221\n",
      "Accuracy:  0.7945575757575758\n",
      "Micro-averaged F1-score:  0.7945575757575758\n"
     ]
    }
   ],
   "source": [
    "# 1gram+2gram, C=5, loss=squared_hinge\n",
    "print(\"Results for 1gram2gram with C=5 and loss=squared_hinge\")\n",
    "svm4 = LinearSVC(C=5, loss = 'squared_hinge', random_state=42)\n",
    "model4 = svm4.fit(train_x_1gram2gram, y_train)\n",
    "pred4 = model4.predict(test_x_1gram2gram)\n",
    "score_metrics(y_test, pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best SVM model for Problem 4\n",
    "with open('best_model_svm.pkl', 'wb') as f:\n",
    "    pickle.dump(model3, f)\n",
    "    \n",
    "with open('tfidf_1gram2gram.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_1gram2gram, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext - need to fix problem with crashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_data(data):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(data['transformed_label'], data['binary_label'], test_size=0.33, random_state=42)\n",
    "#     return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score_metrics(test, pred):\n",
    "#     print(\"Precision:\", precision_score(test, pred))\n",
    "#     print(\"Recall: \", recall_score(test, pred))\n",
    "#     print(\"F1 score: \", f1_score(test, pred))\n",
    "#     print(\"Accuracy: \", accuracy_score(test, pred))\n",
    "#     print(\"Micro-averaged F1-score: \", f1_score(test, pred, average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read data in from CSV\n",
    "# data = pd.read_csv(\"cleaned_reviews.csv\")\n",
    "# data = data.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reference: https://medium.com/@ravindraprasad/build-your-own-text-classification-in-less-than-25-lines-of-code-using-fasttext-dae7229f80f9\n",
    "    \n",
    "# # Each line of the text file contains a list of labels, followed by the corresponding document. \n",
    "# # All the labels start by the __label__ prefix, which is how fastText recognize what is a label or what is a word. \n",
    "# # e.g. __label__1 document_x\n",
    "# data['transformed_label'] = data.apply(lambda i: '__label__' + str(i['binary_label']) + ' ' + str(i['text']), axis = 1)\n",
    "\n",
    "# # Split data into test and train sets\n",
    "# X_train, X_test, y_train, y_test = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = fasttext.train_supervised(X_train.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk import word_tokenize\n",
    "import json\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stars, text, and word length of documents after applying tokenization\n",
    "def pre_process(reviews):\n",
    "    star_ls = []\n",
    "    text_ls = []\n",
    "    doc_len_ls = []\n",
    "    for review in reviews:\n",
    "        dat = json.loads(review)\n",
    "        star = dat['stars']\n",
    "        text = dat['text']\n",
    "        star_ls.append(star)\n",
    "        text_ls.append(text)\n",
    "        # apply nltk word tokenization\n",
    "        doc_len_ls.append(len(word_tokenize(text)))\n",
    "    return star_ls, text_ls, doc_len_ls\n",
    "\n",
    "# Apply nltk word tokenization, convert to lower cases, remove non-alphabetic chars and stopwords\n",
    "def tokenization(text, stopwords):\n",
    "    review_tokenized = [i.lower() for i in word_tokenize(text) if i.isalpha() and i not in stopwords]\n",
    "    return review_tokenized\n",
    "\n",
    "# Clean reviews and save into CSV file\n",
    "def cleaning(stars, texts):\n",
    "    # List of stopwords that do not add much meaning to a sentence\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    texts_tokenized = [tokenization(i, stop_words) for i in texts]\n",
    "    cleaned_df = pd.DataFrame({'star': stars, 'text': texts_tokenized})\n",
    "    # Create binary lables: 1 for ratings > 3, 0 for ratings <= 3. This variable is created to apply logistic regression on multi-label class.\n",
    "    cleaned_df['binary_label'] = cleaned_df.apply(lambda x: 1 if x['star'] > 3 else 0, axis=1)\n",
    "    # Remove NAs\n",
    "    cleaned_df = cleaned_df.dropna()\n",
    "    return cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "with open('tfidf_1gram2gram.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "with open('best_model_svm.pkl', 'rb') as f:\n",
    "    best_svm_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in next 50,000 lines of Yelp reviews for prediction\n",
    "lines = open('yelp_academic_dataset_review.json', encoding=\"utf8\").readlines()[600000:650000]\n",
    "star_ls, text_ls, doc_len_ls = pre_process(lines)\n",
    "cleaned_df = cleaning(star_ls, text_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_vectorizer.fit_transform(text_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_svm_model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = best_svm_model.decision_function(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "        'review': text_ls,\n",
    "        'actual_label': cleaned_df['binary_label'].tolist(),\n",
    "        'predicted_label': pred.tolist(),\n",
    "        'confidence': conf.tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction results saved into json file.\n"
     ]
    }
   ],
   "source": [
    "with open('predcition_results.json', 'w') as f:\n",
    "    json.dump(output, f)\n",
    "print(\"Prediction results saved into json file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
